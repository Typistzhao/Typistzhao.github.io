---
layout: post
comments: true
categories: NLP
--- 

# Transformer in 《Attention is All You Need》
下图摘自论文《Attention is All You Need》&[Transformer博客](https://jalammar.github.io/illustrated-transformer/)
<div align=center>
![Transformer](/images/bert_transformer01.jpg =200x300)
</div>
- 图中的encoder和decoder都有6层
<div align=center>
![Transformer](/images/bert_transformer02.jpg)
</div>
- N代表的是多个抽头，多个抽头的作用是使得一个单词得到不同的关注重心
<div align=center>
![Transformer](/images/bert_transformer03.jpg =200x300)
</div>
- 除了self-attention，结构中该包括了Residual block/Normalize/Feed forward
<div align=center>
![Transformer](/images/bert_transformer04.jpg =200x300)
</div>

# Transformer in 《BERT》 

Transformer结构中编码解码的方向依赖是不一样的,解码时候每个单词会与所有单词产生联系，但是解码的时候一个单词只会与其左边的单词产生关联(这种遮掩操作一般称为MASK)

|Transformer结构|方向|
|------|------|
|Transformer encoder| bidirectional |
| Transformer decoder| left-context-only|

---

Transformer_block in codes(BERT_base)

| 参数名 | 数值 | 含义 |
|------|------|------|
| num_hidden_layers | 12 | Number of hidden layers in the Transformer encoder|
|hidden_size| 768|Size of the encoder layers and the pooler layer|
|num_attention_heads| 12 |Number of attention heads for each attention layer in the Transformer encoder|
|hidden_act| glue | We use a gelu activation (Hendrycks and Gimpel, 2016) rather than the standard relu, following OpenAI GPT.|

```python
  def __init__(self,
               vocab_size,
               hidden_size=768,
               num_hidden_layers=12,
               num_attention_heads=12,
               intermediate_size=3072,
               hidden_act="gelu",
               hidden_dropout_prob=0.1,
               attention_probs_dropout_prob=0.1,
               max_position_embeddings=512,
               type_vocab_size=16,
               initializer_range=0.02):
    """Constructs BertConfig.
    Args:
      vocab_size: Vocabulary size of `inputs_ids` in `BertModel`.
      hidden_size: Size of the encoder layers and the pooler layer.
      num_hidden_layers: Number of hidden layers in the Transformer encoder.
      num_attention_heads: Number of attention heads for each attention layer in
        the Transformer encoder.
      intermediate_size: The size of the "intermediate" (i.e., feed-forward)
        layer in the Transformer encoder.
      hidden_act: The non-linear activation function (function or string) in the
        encoder and pooler.
      hidden_dropout_prob: The dropout probability for all fully connected
        layers in the embeddings, encoder, and pooler.
      attention_probs_dropout_prob: The dropout ratio for the attention
        probabilities.
      max_position_embeddings: The maximum sequence length that this model might
        ever be used with. Typically set this to something large just in case
        (e.g., 512 or 1024 or 2048).
      type_vocab_size: The vocabulary size of the `token_type_ids` passed into
        `BertModel`.
      initializer_range: The stdev of the truncated_normal_initializer for
        initializing all weight matrices.
    """
```

